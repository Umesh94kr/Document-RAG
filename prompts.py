from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from langchain.prompts import PromptTemplate

qa_prompt = PromptTemplate(
    input_variables=["context"],
    template="""
You are a question-answering chat assistant. Look at the query and context carefully if query is irrelevant to the retrieved context then return LLM reponse as no context matched, else create the response.

Return the answer in JSON format with the key 'output'. 

For example:

{{
    "output": answer generated by LLM
}}

{context}
""".strip()
)